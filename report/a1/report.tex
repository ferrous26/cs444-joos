\documentclass[pdftex,11pt,a4paper]{article}
\usepackage{../cs444}

\begin{document}

\titlemake{1}

This document outlines the design of a scanner, parser, and weeder for
the Joos 1W language as specified by the Java Language Specification
2.0 (JLS) and the course website
\url{https://www.student.cs.uwaterloo.ca/~cs444/joos.html}. It also
discusses some challenges faced while implementing the scanner and
parser, as well as our strategy for testing correctness.

Code is written in Ruby, and our implementation code is located in
\ttt{lib/} of the submitted code. A copy of the Marmoset tests is
located in \ttt{test/}, while our own tests are located in
\ttt{spec/}. Miscellaneous tasks are handled by the Ruby \ttt{make}
(\ttt{rake}) system, which is organzied in \ttt{rakelib/}.


\section{Design}

The design of this phase of the compiler is broken up into five
sections: a description of our token data structures which are
produced by the scanner and consumed by the parser; the scanner, which
produces tokens from input source code; the grammar specification,
which are used to generate the parser rules and concrete syntax tree
classes; the parser, which produces a concrete syntax tree (CST) from
the tokenized input; and a weeding phase, which begins to transform
the CST into an abstract syntax tree (AST) while performing some
grammar checks that would have been difficult to do during parsing.


\subsection{Tokens}

The interface between the scanner and the parser is an \ttt{Array} of
\ttt{Joos::Token} objects. \ttt{Joos::Token} is an abstract base class
which encodes the original value of the token, the type of the token,
and metadata related to where the token originated in the source file.

Concrete subclasses are created for each keyword, operator, separator,
and literal type, as defined in the JLS. A concrete class for
identifiers is also created in the \ttt{Joos::Token}
namespace. Classes may \ttt{include} mixins which attach extra
metadata to a token, though the name of the concrete class specifies
the type of the token.

For instance, a token representing the keyword \ttt{while} will
be contained by an instance of the \ttt{Joos::Token::While} class, and
a token representing a litertal \ttt{true} will be contained by an
instance of the \ttt{Joos::Token::True} class. Identifiers are wrapped
by \ttt{Joos::Token::Identifier}, literal integers by
\ttt{Joos::Token::Integer}, and so on.

Each token contains metadata that can be used for diagnostic purposes
including, but not limited to, the path to the source file where the
token originated, as well as line and column numbers where the token
begins. Additional metadata is attached to different groupings of token classes to create
a sort of hierarchy. Additional metadata include things like the
\ttt{IllegalToken} modifier which is used to mark the types of tokens
that are legal Java tokens, but are not used in Joos.

Finally, concrete token classes will validate token values if
needed. In the case of string and character literals, we check that
all escape sequences in the string are legal escape sequences. Token
classes which are marked as illegal will raise an exception in their
constructor, which will cause the compiler to print an appropriate
error message and then exit.


\subsection{Lexer}

The lexical analysis portion of our compiler consists of 3 classes: 
\ttt{Scanner}, which simply feeds lines into the \ttt{tokenize}
function; \ttt{DFA}, a general deterministic finite automaton class;
and \ttt{ScannerDFA}, a subclass of \ttt{DFA} which contains the
Joos-specific lexing rules.

\ttt{DFA} contains the structure of a finite automaton: its
transition table and accepting states, with an implicit start symbol
named \ttt{:start}. The transition table maps a state symbol to an
array of \ttt{(predicate, state)} pairs. The \ttt{transition} function
looks up a transition from a state on a given input character by
first calling \ttt{classify} to map the character to the DFA's alphabet.
\ttt{classify} is overridden in \ttt{ScannerDFA} to raise an exception
if the input is non-ASCII. \ttt{transition} looks up the state in the
transition table and checks each predicate in order aginst the input
to determine which state to follow. Invalid transitions return the
state \ttt{:error}. Not all valid states need be in the table --
we have accepting states like \ttt{:line\_comment} that contain
no transitions, as well as error states like \ttt{:float} for
catching certain illegal tokens.

To perform tokenization, \ttt{DFA} has a function \ttt{tokenize}
which runs the maximal munch algorithm against an input string. The
algorithm doesn't implement backtracking, since it is unnecessary.
\ttt{tokenize} returns 2 values: an array of \ttt{DFA::Token}s, which
are simple structures that later get mapped to full \ttt{Joos::Token}s,
and an \ttt{AutomatonState}, which represents the final state of the
algorithm. \ttt{AutomatonState} is a nested class that holds the internal
state of a run through the DFA -- the current state symbol and input seen so
far. Tokenization is performed line-by-line (except in tests). The
returned \ttt{AutomatonState} `continuation' is passed to the next
call to \ttt{tokenize} to handle the case of block comments, which may
span multiple lines.

The use of a continuation state, however, requires us to implement two
addition \ttt{ScannerDFA} methods: \ttt{raise\_if\_illegal\_line\_end!} and 
\ttt{raise\_if\_illegal\_eof!}. These methods check that we haven't
ended on any incomplete tokens.

\ttt{DFA} also has a few convenience methods for initializing the
transition table. The principal method is \ttt{state} which provides a
Ruby DSL (implemented in the \ttt{StateBuilder} nested class) for
concisely specifying a state and its transitions.

One simplifying design choice we made is to scan keywords and identifiers
using the same rule and only differentiate between them when later
constructing \ttt{Token}s.

\subsection{Abstract Syntax Tree}

Our goal with the abstract syntax tree is to provide a data structure
that will make future stages of the compiler easier to implement.


\subsection{Parser}

It takes a stream of tokens from the lexer and does some other stuff.


\section{Challenges}


This is where the interesting shit goes. Talk about anything that did
not follow simply from class. This is a required section of the report.

\subsection{Lexer}

In the lexer, one challenge was the process of building the transition
table. Originally, the \ttt{DFA} class had a nested map from states
to input symbols to transition states. The \ttt{classify} function
mapped some characters to character classes (symbols) in order to avoid
having many similar transitions. This approach, however, became
unwieldy for strings and comments, which needed a transition for
essentially every character. Changing the map from inputs to states
into an ordered array of \ttt{(predicate, state)} pairs greatly simplified
the rules for strings and comments by having a few transitions for
exceptional cases then a final transition for everything else.


\subsection{Weeder}

Weeding is performed during both scanning and parsing. Our goal is to
weed out issues as early as possible in order to make later stages
easier to implement, and so most weeding is done during scanning.

All keywords and operators not supported by Joos are caught during
scanning and will never be passed to the parser. Literal floating
point values and unsupported formats of literal integers are also
caught during the scanning phase.

However, validating that literal integers are within the signed 32-bit
range cannot be done during scanning because the range of values
differs depending on if the integer has a negative or positive value,
and we cannot determine that information until the tokens have been
parsed.


\subsection{Duplicate String Literals}

Literal string duplication is avoided by overriding how
\ttt{Joos::Token::String} objects are allocated. We override the
default allocation method for the class and check if another string
with the same binary value already exists. If it the string already
exists we return the existing token instead of a new token. The trade
off here is that we lose any metadata attached to the duplicate
literal string, such as the file and line where the string came from;
so any compilation errors related to the string may point to the
incorrect string, depending on the type of error. We compare against
the binary form of the string so that differences in escape sequences
do not affect the detection of duplicate strings.

I should probably modify this strategy a bit so that we actually keep
duplicate literal strings during the analysis phase of compilation,
and just link their in memory address when it comes to code
generation.


\section{Testing}

We wrote tests. And we also have an offline version of marmoset tests
which we run in the student environment at roflscale.

This section is required, so we have to fill it out at some point\ldots


\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
